"""
EasyDict API Service
æä¾›è¯å…¸æŸ¥è¯¢ã€åˆ—è¡¨è·å–å’Œä¸‹è½½æœåŠ¡
"""

import os
import json
import logging
import sqlite3
import zipfile
import time
from pathlib import Path
from contextlib import asynccontextmanager
from typing import Optional, List, Dict, Any
from datetime import datetime
from contextlib import contextmanager

import aiosqlite
import zstandard as zstd
from fastapi import FastAPI, HTTPException, Request, Query
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# é…ç½®æ—¥å¿—
LOG_LEVEL = os.getenv("LOG_LEVEL", "info").upper()
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@contextmanager
def performance_timer(operation_name: str, log_level: int = logging.INFO):
    """
    æ€§èƒ½æµ‹é‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨

    ç”¨æ³•:
        with performance_timer("æ“ä½œåç§°"):
            # ä½ çš„ä»£ç 

    è¾“å‡º:
        [PERF] æ“ä½œåç§°: 123.45ms
    """
    start_time = time.perf_counter()
    try:
        yield
    finally:
        elapsed_ms = (time.perf_counter() - start_time) * 1000
        logger.log(log_level, f"[PERF] {operation_name}: {elapsed_ms:.2f}ms")

# è¯å…¸æ•°æ®æ ¹ç›®å½•
DICTIONARIES_PATH = Path(os.getenv("DICTIONARIES_PATH", "/data/dictionaries"))
# è¾…åŠ©æ•°æ®æ–‡ä»¶ç›®å½•
AUXILIARY_PATH = Path(os.getenv("AUXILIARY_PATH", "/data/auxiliary"))
CACHE_PATH = Path(os.getenv("CACHE_PATH", "/tmp/easydict-cache"))

# æ•°æ®åº“è¿æ¥ç¼“å­˜
_db_connections: Dict[str, aiosqlite.Connection] = {}
# Media æ•°æ®åº“è¿æ¥ç¼“å­˜
_media_db_connections: Dict[str, aiosqlite.Connection] = {}
# Zstd è§£å‹å™¨ç¼“å­˜ï¼ˆæ¯ä¸ªè¯å…¸ä¸€ä¸ªï¼Œä» config è¡¨çš„ zstd_dict åŠ è½½ï¼‰
_zstd_decompressors: Dict[str, zstd.ZstdDecompressor] = {}

# ZIP æ–‡ä»¶å¯¹è±¡ç¼“å­˜
# ç¼“å­˜æ‰“å¼€çš„ ZipFile å¯¹è±¡ï¼Œé¿å…æ¯æ¬¡è¯·æ±‚éƒ½é‡æ–°æ‰“å¼€ ZIP æ–‡ä»¶
_zip_file_cache: Dict[str, zipfile.ZipFile] = {}
# ZIP æ–‡ä»¶ç´¢å¼•ç¼“å­˜
# æ ¼å¼: {zip_path: {request_path: actual_path_in_zip}}
_zip_index_cache: Dict[str, Dict[str, str]] = {}
# ZIP æ–‡ä»¶ä¿®æ”¹æ—¶é—´ç¼“å­˜ï¼ˆç”¨äºæ£€æµ‹æ–‡ä»¶æ›´æ–°ï¼‰
_zip_file_mtime_cache: Dict[str, float] = {}
# ZIP æ–‡ä»¶æ•°é‡ç¼“å­˜ï¼ˆç›´æ¥å­˜å‚¨æ–‡ä»¶æ•°é‡ï¼ŒO(1) æŸ¥è¯¢ï¼‰
_zip_file_count_cache: Dict[str, int] = {}

# ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
CACHE_PATH.mkdir(parents=True, exist_ok=True)



class DictionaryFileInfo(BaseModel):
    """è¯å…¸æ–‡ä»¶ä¿¡æ¯"""
    name: str
    size: int
    modified: float


class DictionaryInfo(BaseModel):
    """è¯å…¸ä¿¡æ¯æ¨¡å‹"""
    id: str
    name: str
    version: int = 1
    entry_count: int = 0
    audio_count: int = 0
    image_count: int = 0
    dict_size: int = 0
    media_size: int = 0
    updated_at: Optional[str] = None


async def preload_zip_indexes():
    """
    é¢„åŠ è½½æ‰€æœ‰è¯å…¸çš„ ZIP æ–‡ä»¶ç´¢å¼•

    åœ¨åº”ç”¨å¯åŠ¨æ—¶æ‰«ææ‰€æœ‰è¯å…¸ç›®å½•ï¼Œä¸ºæ¯ä¸ªå­˜åœ¨çš„ audios.zip å’Œ images.zip
    é¢„æ„å»ºç´¢å¼•å¹¶ç¼“å­˜ã€‚è¿™æ ·å¯ä»¥é¿å…é¦–æ¬¡è®¿é—®éŸ³é¢‘/å›¾ç‰‡æ–‡ä»¶æ—¶çš„å»¶è¿Ÿã€‚

    æ€§èƒ½å½±å“:
    - åº”ç”¨å¯åŠ¨æ—¶é—´ä¼šå¢åŠ ï¼ˆå–å†³äº ZIP æ–‡ä»¶å¤§å°å’Œæ•°é‡ï¼‰
    - ä½†é¦–æ¬¡è®¿é—®éŸ³é¢‘/å›¾ç‰‡æ–‡ä»¶æ—¶ä» 2000ms é™åˆ° <1ms
    """
    if not DICTIONARIES_PATH.exists():
        logger.warning(f"Dictionaries path does not exist: {DICTIONARIES_PATH}")
        return

    logger.info("=" * 60)
    logger.info("å¼€å§‹é¢„åŠ è½½ ZIP æ–‡ä»¶ç´¢å¼•...")
    logger.info("=" * 60)

    preload_start = time.perf_counter()
    total_indexes = 0
    total_files = 0

    try:
        # æ‰«ææ‰€æœ‰è¯å…¸ç›®å½•
        for dict_path in DICTIONARIES_PATH.iterdir():
            if not dict_path.is_dir():
                continue

            dict_id = dict_path.name
            logger.info(f"å¤„ç†è¯å…¸: {dict_id}")

            # æ£€æŸ¥å¹¶é¢„åŠ è½½ audios.zip
            audios_zip = dict_path / "audios.zip"
            if audios_zip.exists():
                try:
                    logger.info(f"  - é¢„åŠ è½½éŸ³é¢‘ç´¢å¼•: {audios_zip.name}")
                    index_start = time.perf_counter()
                    zip_index = await get_zip_index(audios_zip)
                    index_ms = (time.perf_counter() - index_start) * 1000
                    total_indexes += 1
                    total_files += len(zip_index)
                    logger.info(f"    âœ“ å®Œæˆ | ç´¢å¼•æ¡ç›®: {len(zip_index):,} | è€—æ—¶: {index_ms:.2f}ms")
                except Exception as e:
                    logger.error(f"    âœ— å¤±è´¥: {e}")

            # æ£€æŸ¥å¹¶é¢„åŠ è½½ images.zip
            images_zip = dict_path / "images.zip"
            if images_zip.exists():
                try:
                    logger.info(f"  - é¢„åŠ è½½å›¾ç‰‡ç´¢å¼•: {images_zip.name}")
                    index_start = time.perf_counter()
                    zip_index = await get_zip_index(images_zip)
                    index_ms = (time.perf_counter() - index_start) * 1000
                    total_indexes += 1
                    total_files += len(zip_index)
                    logger.info(f"    âœ“ å®Œæˆ | ç´¢å¼•æ¡ç›®: {len(zip_index):,} | è€—æ—¶: {index_ms:.2f}ms")
                except Exception as e:
                    logger.error(f"    âœ— å¤±è´¥: {e}")

    except Exception as e:
        logger.error(f"é¢„åŠ è½½ ZIP ç´¢å¼•æ—¶å‡ºé”™: {e}")

    total_ms = (time.perf_counter() - preload_start) * 1000

    logger.info("=" * 60)
    logger.info(f"ZIP ç´¢å¼•é¢„åŠ è½½å®Œæˆï¼")
    logger.info(f"  - é¢„åŠ è½½ç´¢å¼•æ•°: {total_indexes}")
    logger.info(f"  - æ€»ç´¢å¼•æ¡ç›®: {total_files:,}")
    logger.info(f"  - æ€»è€—æ—¶: {total_ms:.2f}ms ({total_ms/1000:.2f}ç§’)")
    logger.info("=" * 60)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info(f"Starting EasyDict API Server")
    logger.info(f"Dictionaries path: {DICTIONARIES_PATH}")
    logger.info(f"Auxiliary path: {AUXILIARY_PATH}")
    logger.info(f"Cache path: {CACHE_PATH}")

    # é¢„åŠ è½½æ‰€æœ‰ ZIP æ–‡ä»¶ç´¢å¼•
    await preload_zip_indexes()

    yield
    # æ¸…ç†æ•°æ®åº“è¿æ¥
    for conn in _db_connections.values():
        await conn.close()
    # æ¸…ç† media æ•°æ®åº“è¿æ¥
    for conn in _media_db_connections.values():
        await conn.close()
    # å…³é—­æ‰€æœ‰æ‰“å¼€çš„ ZipFile å¯¹è±¡
    for zf in _zip_file_cache.values():
        try:
            zf.close()
        except:
            pass
    # æ¸…ç† ZIP ç¼“å­˜
    _zip_file_cache.clear()
    _zip_index_cache.clear()
    _zip_file_count_cache.clear()
    _zip_file_mtime_cache.clear()
    # æ¸…ç† zstd è§£å‹å™¨ç¼“å­˜
    _zstd_decompressors.clear()
    logger.info("EasyDict API Server stopped")


app = FastAPI(
    title="EasyDict API",
    description="è¯å…¸æŸ¥è¯¢å’Œä¸‹è½½æœåŠ¡ API",
    version="2.0.0",
    lifespan=lifespan
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)


async def get_db_connection(dict_id: str) -> Optional[aiosqlite.Connection]:
    """è·å–è¯å…¸æ•°æ®åº“è¿æ¥ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    cache_key = dict_id

    if cache_key in _db_connections:
        return _db_connections[cache_key]

    db_path = DICTIONARIES_PATH / dict_id / "dictionary.db"

    if not db_path.exists():
        return None

    try:
        conn = await aiosqlite.connect(str(db_path))
        conn.row_factory = aiosqlite.Row
        _db_connections[cache_key] = conn
        return conn
    except Exception as e:
        logger.error(f"Failed to connect to database {db_path}: {e}")
        return None


async def get_db_connection_raw(dict_id: str) -> Optional[aiosqlite.Connection]:
    """è·å–è¯å…¸æ•°æ®åº“è¿æ¥ï¼ˆä¸å¸¦ row_factoryï¼Œç”¨äºèšåˆæŸ¥è¯¢ï¼‰"""
    db_path = DICTIONARIES_PATH / dict_id / "dictionary.db"

    if not db_path.exists():
        return None

    try:
        conn = await aiosqlite.connect(str(db_path))
        return conn
    except Exception as e:
        logger.error(f"Failed to connect to database {db_path}: {e}")
        return None


async def get_media_db_connection(dict_id: str) -> Optional[aiosqlite.Connection]:
    """è·å– media.db æ•°æ®åº“è¿æ¥ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    cache_key = f"media_{dict_id}"

    if cache_key in _media_db_connections:
        return _media_db_connections[cache_key]

    media_db_path = DICTIONARIES_PATH / dict_id / "media.db"

    if not media_db_path.exists():
        return None

    try:
        conn = await aiosqlite.connect(str(media_db_path))
        conn.row_factory = aiosqlite.Row
        _media_db_connections[cache_key] = conn
        return conn
    except Exception as e:
        logger.error(f"Failed to connect to media database {media_db_path}: {e}")
        return None


async def create_media_db(dict_id: str) -> bool:
    """
    åˆ›å»º media.db æ•°æ®åº“å’Œè¡¨ç»“æ„

    Args:
        dict_id: è¯å…¸ID

    Returns:
        æ˜¯å¦æˆåŠŸåˆ›å»º
    """
    media_db_path = DICTIONARIES_PATH / dict_id / "media.db"

    try:
        async with aiosqlite.connect(str(media_db_path)) as conn:
            # åˆ›å»º audios è¡¨
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS audios (
                    name TEXT PRIMARY KEY,
                    blob BLOB NOT NULL
                )
            """)

            # åˆ›å»º images è¡¨
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS images (
                    name TEXT PRIMARY KEY,
                    blob BLOB NOT NULL
                )
            """)

            # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
            await conn.execute("CREATE INDEX IF NOT EXISTS idx_audios_name ON audios(name)")
            await conn.execute("CREATE INDEX IF NOT EXISTS idx_images_name ON images(name)")

            await conn.commit()

        logger.info(f"Created media.db for dictionary '{dict_id}'")
        return True

    except Exception as e:
        logger.error(f"Failed to create media.db for {dict_id}: {e}")
        return False


async def migrate_zip_to_media_db(dict_id: str) -> bool:
    """
    å°† audios.zip å’Œ images.zip ä¸­çš„æ–‡ä»¶è¿ç§»åˆ° media.db

    Args:
        dict_id: è¯å…¸ID

    Returns:
        æ˜¯å¦æˆåŠŸè¿ç§»
    """
    dict_path = DICTIONARIES_PATH / dict_id
    audios_zip = dict_path / "audios.zip"
    images_zip = dict_path / "images.zip"

    # æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦è¿ç§»çš„æ–‡ä»¶
    if not audios_zip.exists() and not images_zip.exists():
        logger.info(f"No zip files to migrate for dictionary '{dict_id}'")
        return True

    try:
        # é¦–å…ˆåˆ›å»ºæ•°æ®åº“
        if not await create_media_db(dict_id):
            logger.error(f"Failed to create media.db for {dict_id}")
            return False

        # è·å–æ•°æ®åº“è¿æ¥
        conn = await get_media_db_connection(dict_id)
        if not conn:
            logger.error(f"Failed to connect to media.db for {dict_id}")
            return False

        # è¿ç§»éŸ³é¢‘æ–‡ä»¶
        if audios_zip.exists():
            logger.info(f"Migrating audios.zip to media.db for '{dict_id}'")
            try:
                with zipfile.ZipFile(audios_zip, 'r') as zf:
                    for file_info in zf.filelist:
                        if file_info.is_dir():
                            continue

                        file_name = file_info.filename
                        # åªä½¿ç”¨æ–‡ä»¶åï¼Œä¸åŒ…å«è·¯å¾„
                        simple_name = file_name.split('/')[-1]

                        # è¯»å–æ–‡ä»¶å†…å®¹
                        file_data = zf.read(file_name)

                        # æ’å…¥æ•°æ®åº“
                        await conn.execute(
                            "INSERT OR REPLACE INTO audios (name, blob) VALUES (?, ?)",
                            (simple_name, file_data)
                        )

                await conn.commit()
                logger.info(f"Successfully migrated audios from '{dict_id}'")
            except Exception as e:
                logger.error(f"Failed to migrate audios for {dict_id}: {e}")
                return False

        # è¿ç§»å›¾ç‰‡æ–‡ä»¶
        if images_zip.exists():
            logger.info(f"Migrating images.zip to media.db for '{dict_id}'")
            try:
                with zipfile.ZipFile(images_zip, 'r') as zf:
                    for file_info in zf.filelist:
                        if file_info.is_dir():
                            continue

                        file_name = file_info.filename
                        # åªä½¿ç”¨æ–‡ä»¶åï¼Œä¸åŒ…å«è·¯å¾„
                        simple_name = file_name.split('/')[-1]

                        # è¯»å–æ–‡ä»¶å†…å®¹
                        file_data = zf.read(file_name)

                        # æ’å…¥æ•°æ®åº“
                        await conn.execute(
                            "INSERT OR REPLACE INTO images (name, blob) VALUES (?, ?)",
                            (simple_name, file_data)
                        )

                await conn.commit()
                logger.info(f"Successfully migrated images from '{dict_id}'")
            except Exception as e:
                logger.error(f"Failed to migrate images for {dict_id}: {e}")
                return False

        logger.info(f"Successfully migrated media files for dictionary '{dict_id}'")
        return True

    except Exception as e:
        logger.error(f"Failed to migrate media files for {dict_id}: {e}")
        return False


async def count_files_in_media_db(dict_id: str, table_name: str) -> int:
    """
    ç»Ÿè®¡ media.db ä¸­çš„æ–‡ä»¶æ•°é‡

    Args:
        dict_id: è¯å…¸ID
        table_name: è¡¨å ('audios' æˆ– 'images')

    Returns:
        æ–‡ä»¶æ•°é‡
    """
    conn = await get_media_db_connection(dict_id)
    if not conn:
        return 0

    try:
        cursor = await conn.execute(f"SELECT COUNT(*) as count FROM {table_name}")
        row = await cursor.fetchone()
        return row[0] if row else 0
    except Exception as e:
        logger.error(f"Failed to count files in {table_name}: {e}")
        return 0


async def get_zip_index(zip_path: Path) -> Dict[str, str]:
    """
    è·å– ZIP æ–‡ä»¶è·¯å¾„ç´¢å¼•ï¼ˆå¸¦ç¼“å­˜ï¼‰

    æ„å»º {è¯·æ±‚è·¯å¾„: ZIPå†…å®é™…è·¯å¾„} çš„æ˜ å°„å­—å…¸
    ä¾‹å¦‚: {'word.mp3': 'audios/word.mp3', 'word': 'audios/word.mp3'}

    æ€§èƒ½ä¼˜åŒ–:
    - é¦–æ¬¡è°ƒç”¨: æ‰“å¼€ ZIP æ–‡ä»¶ï¼Œè§£æ central directoryï¼Œæ„å»ºç´¢å¼•ï¼Œå¹¶ä¿æŒ ZipFile å¯¹è±¡æ‰“å¼€
    - åç»­è°ƒç”¨: ç›´æ¥ä½¿ç”¨å·²æ‰“å¼€çš„ ZipFile å¯¹è±¡å’Œç¼“å­˜çš„ç´¢å¼•ï¼ŒO(1) æŸ¥æ‰¾
    - è‡ªåŠ¨æ›´æ–°: æ£€æµ‹æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼Œå˜åŒ–æ—¶é‡æ–°æ‰“å¼€ ZIP å¹¶é‡å»ºç´¢å¼•

    Args:
        zip_path: ZIP æ–‡ä»¶è·¯å¾„

    Returns:
        è·¯å¾„æ˜ å°„å­—å…¸ {request_path: actual_path_in_zip}
    """
    total_start = time.perf_counter()

    cache_key = str(zip_path)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    with performance_timer(f"æ£€æŸ¥ ZIP æ–‡ä»¶å­˜åœ¨: {zip_path.name}"):
        if not zip_path.exists():
            raise HTTPException(status_code=404, detail=f"Zip file not found: {zip_path}")

    # è·å–å½“å‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´
    try:
        with performance_timer(f"è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´: {zip_path.name}"):
            current_mtime = zip_path.stat().st_mtime
    except Exception as e:
        logger.error(f"Failed to get mtime for {zip_path}: {e}")
        raise HTTPException(status_code=500, detail="Failed to access zip file")

    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    cache_check_start = time.perf_counter()
    if cache_key in _zip_file_cache:
        if _zip_file_mtime_cache.get(cache_key) == current_mtime:
            # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›
            cache_check_ms = (time.perf_counter() - cache_check_start) * 1000
            logger.info(f"[PERF] ZIP æ–‡ä»¶å¯¹è±¡ç¼“å­˜å‘½ä¸­: {cache_check_ms:.2f}ms")
            logger.debug(f"ZIP file cache hit for {zip_path.name}")
            return _zip_index_cache[cache_key]
        else:
            logger.info(f"ZIP file {zip_path.name} has been modified, reopening...")
            # å…³é—­æ—§çš„ ZipFile
            try:
                _zip_file_cache[cache_key].close()
            except:
                pass
            del _zip_file_cache[cache_key]

    # ç¼“å­˜æœªå‘½ä¸­æˆ–æ–‡ä»¶å·²æ›´æ–°ï¼Œé‡æ–°æ‰“å¼€ ZIP å¹¶æ„å»ºç´¢å¼•
    logger.info(f"Opening ZIP file and building index for {zip_path.name}...")

    index = {}  # {è¯·æ±‚è·¯å¾„: å®é™…è·¯å¾„}
    file_count = 0  # å®é™…æ–‡ä»¶æ•°é‡ï¼ˆæ’é™¤ç›®å½•ï¼‰

    try:
        with performance_timer(f"æ‰“å¼€å¹¶è§£æ ZIP æ–‡ä»¶: {zip_path.name}"):
            # æ‰“å¼€ ZIP æ–‡ä»¶å¹¶ä¿æŒæ‰“å¼€çŠ¶æ€
            zf = zipfile.ZipFile(zip_path, 'r')

            with performance_timer(f"éå† ZIP æ–‡ä»¶åˆ—è¡¨"):
                namelist = zf.namelist()

            with performance_timer(f"æ„å»ºç´¢å¼•æ˜ å°„ ({len(namelist)} ä¸ªæ–‡ä»¶)"):
                # éå† ZIP ä¸­çš„æ‰€æœ‰æ–‡ä»¶
                for name in namelist:
                    if name.endswith('/'):  # è·³è¿‡ç›®å½•é¡¹
                        continue

                    # ç»Ÿè®¡å®é™…æ–‡ä»¶æ•°é‡
                    file_count += 1

                    # ä¸ºæ¯ä¸ªæ–‡ä»¶åˆ›å»ºå¤šä¸ªè®¿é—®è·¯å¾„çš„æ˜ å°„
                    # è¿™æ ·æ— è®ºå®¢æˆ·ç«¯è¯·æ±‚ä»€ä¹ˆè·¯å¾„æ ¼å¼éƒ½èƒ½æ‰¾åˆ°

                    # 1. å®Œæ•´è·¯å¾„æ˜ å°„
                    index[name] = name

                    # 2. åªæœ‰æ–‡ä»¶åçš„æ˜ å°„ï¼ˆæœ€å¸¸ç”¨ï¼‰
                    filename = name.split('/')[-1]
                    if filename:  # ç¡®ä¿æ–‡ä»¶åä¸ä¸ºç©º
                        index[filename] = name

                    # 3. å»é™¤å¯èƒ½çš„ç›®å½•å‰ç¼€
                    parts = name.split('/')
                    if len(parts) > 1:
                        # å»é™¤ç¬¬ä¸€å±‚ç›®å½•å‰ç¼€
                        short_path = '/'.join(parts[1:])
                        index[short_path] = name

                        # å»é™¤ä¸¤å±‚ç›®å½•å‰ç¼€ï¼ˆåŒé‡å‰ç¼€æƒ…å†µï¼‰
                        if len(parts) > 2:
                            short_path2 = '/'.join(parts[2:])
                            index[short_path2] = name

        # ç¼“å­˜ ZipFile å¯¹è±¡ã€ç´¢å¼•ã€æ–‡ä»¶æ•°é‡å’Œä¿®æ”¹æ—¶é—´
        with performance_timer("ç¼“å­˜ ZipFile å¯¹è±¡å’Œç´¢å¼•åˆ°å†…å­˜"):
            _zip_file_cache[cache_key] = zf  # ä¿æŒ ZipFile æ‰“å¼€ï¼
            _zip_index_cache[cache_key] = index
            _zip_file_count_cache[cache_key] = file_count  # ç¼“å­˜æ–‡ä»¶æ•°é‡
            _zip_file_mtime_cache[cache_key] = current_mtime

        total_ms = (time.perf_counter() - total_start) * 1000
        logger.info(f"[PERF] ZIP æ–‡ä»¶æ‰“å¼€å’Œç´¢å¼•æ„å»ºæ€»è€—æ—¶: {total_ms:.2f}ms | æ–‡ä»¶: {zip_path.name} | æ–‡ä»¶æ•°: {file_count} | ç´¢å¼•æ¡ç›®: {len(index)}")
        return index

    except zipfile.BadZipFile as e:
        logger.error(f"Invalid ZIP file {zip_path}: {e}")
        raise HTTPException(status_code=500, detail=f"Invalid ZIP file: {str(e)}")
    except Exception as e:
        logger.error(f"Failed to open ZIP file {zip_path}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to open ZIP file: {str(e)}")


def parse_json_field(value: Optional[str]) -> Any:
    """è§£æ JSON å­—æ®µ"""
    if value is None:
        return None
    try:
        return json.loads(value)
    except json.JSONDecodeError:
        return value


async def get_zstd_decompressor(dict_id: str) -> Optional[zstd.ZstdDecompressor]:
    """è·å–è¯å…¸çš„ zstd è§£å‹å™¨ï¼ˆå¸¦ç¼“å­˜ï¼‰ï¼Œä» config è¡¨è¯»å–å‹ç¼©å­—å…¸"""
    if dict_id in _zstd_decompressors:
        return _zstd_decompressors[dict_id]

    conn = await get_db_connection(dict_id)
    if conn is None:
        return None

    try:
        cursor = await conn.execute(
            "SELECT value FROM config WHERE key = 'zstd_dict'"
        )
        row = await cursor.fetchone()
        await cursor.close()
        if row and row[0]:
            zdict = zstd.ZstdCompressionDict(bytes(row[0]))
            dctx = zstd.ZstdDecompressor(dict_data=zdict)
            _zstd_decompressors[dict_id] = dctx
            return dctx
    except Exception as e:
        logger.warning(f"No zstd dict for '{dict_id}', falling back to plain decompressor: {e}")

    # Fallback: no training dict
    dctx = zstd.ZstdDecompressor()
    _zstd_decompressors[dict_id] = dctx
    return dctx


def decompress_json_data(data: bytes, dctx: Optional[zstd.ZstdDecompressor]) -> Any:
    """è§£å‹ json_data BLOB å¹¶è§£æä¸ºå¯¹è±¡ï¼›è‹¥è§£å‹å¤±è´¥åˆ™ç›´æ¥ json.loads"""
    if data is None:
        return {}
    raw = bytes(data)
    # Try zstd decompression
    if dctx is not None:
        try:
            raw = dctx.decompress(raw)
        except Exception:
            pass  # Not compressed (or wrong dict) â€“ use as-is
    try:
        return json.loads(raw)
    except Exception:
        return {}



def get_directory_size(path: Path) -> int:
    """è·å–ç›®å½•æ€»å¤§å°"""
    total = 0
    if path.exists():
        for item in path.rglob('*'):
            if item.is_file():
                total += item.stat().st_size
    return total


def count_files_in_directory(path: Path) -> int:
    """ç»Ÿè®¡ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶æ•°é‡ï¼ˆä¸åŒºåˆ†æ‰©å±•åï¼‰"""
    if not path.exists():
        return 0
    count = 0
    for item in path.iterdir():
        if item.is_file():
            count += 1
    return count


async def count_files_in_zip_index(zip_path: Path) -> int:
    """
    ä»ç¼“å­˜ä¸­è·å– ZIP æ–‡ä»¶æ•°é‡ï¼ˆO(1) æŸ¥è¯¢ï¼‰

    æ€§èƒ½ä¼˜åŒ–ï¼šæ–‡ä»¶æ•°é‡åœ¨é¢„åŠ è½½ ZIP ç´¢å¼•æ—¶å·²ç»ç»Ÿè®¡å¹¶ç¼“å­˜ï¼Œç›´æ¥è¯»å–å³å¯

    Args:
        zip_path: ZIP æ–‡ä»¶è·¯å¾„

    Returns:
        æ–‡ä»¶æ•°é‡
    """
    cache_key = str(zip_path)

    # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰è¯¥ ZIP æ–‡ä»¶çš„æ–‡ä»¶æ•°é‡
    if cache_key in _zip_file_count_cache:
        # O(1) ç›´æ¥ä»ç¼“å­˜è¯»å–æ–‡ä»¶æ•°é‡
        return _zip_file_count_cache[cache_key]

    # ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œå…ˆåŠ è½½ç´¢å¼•ï¼ˆä¼šè‡ªåŠ¨ç»Ÿè®¡å¹¶ç¼“å­˜æ–‡ä»¶æ•°é‡ï¼‰
    try:
        await get_zip_index(zip_path)
        # åŠ è½½å®Œæˆåï¼Œæ–‡ä»¶æ•°é‡å·²ç»ç¼“å­˜
        return _zip_file_count_cache.get(cache_key, 0)
    except Exception as e:
        # ZIP æ–‡ä»¶ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥
        logger.error(f"Failed to load ZIP index for {zip_path.name}: {e}")
        return 0


async def get_dictionary_info(dict_id: str) -> Optional[DictionaryInfo]:
    """è·å–è¯å…¸è¯¦ç»†ä¿¡æ¯"""
    dict_path = DICTIONARIES_PATH / dict_id
    
    if not dict_path.exists() or not dict_path.is_dir():
        return None
    
    metadata = {}
    metadata_path = dict_path / "metadata.json"
    if metadata_path.exists():
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        except Exception as e:
            logger.warning(f"Failed to read metadata for {dict_id}: {e}")
    
    db_path = dict_path / "dictionary.db"
    media_db_path = dict_path / "media.db"
    audios_path = dict_path / "audios"
    images_path = dict_path / "images"

    entry_count = 0
    if db_path.exists():
        try:
            conn = await get_db_connection_raw(dict_id)
            if conn:
                cursor = await conn.execute("SELECT COUNT(DISTINCT headword) as count FROM entries")
                row = await cursor.fetchone()
                entry_count = row[0] if row else 0
                await conn.close()
        except Exception as e:
            logger.warning(f"Failed to count entries for {dict_id}: {e}")

    dict_size = db_path.stat().st_size if db_path.exists() else 0
    media_size = media_db_path.stat().st_size if media_db_path.exists() else 0

    audio_count = 0
    if media_db_path.exists():
        audio_count = await count_files_in_media_db(dict_id, 'audios')
    elif audios_path.exists():
        audio_count = count_files_in_directory(audios_path)

    image_count = 0
    if media_db_path.exists():
        image_count = await count_files_in_media_db(dict_id, 'images')
    elif images_path.exists():
        image_count = count_files_in_directory(images_path)

    stat = dict_path.stat()
    updated_at = datetime.fromtimestamp(stat.st_mtime).isoformat()

    return DictionaryInfo(
        id=dict_id,
        name=metadata.get('name', dict_id),
        version=metadata.get('version', 1),
        entry_count=entry_count,
        audio_count=audio_count,
        image_count=image_count,
        dict_size=dict_size,
        media_size=media_size,
        updated_at=updated_at
    )


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "healthy", "service": "easydict-api"}


ALLOWED_FILES = {
    "logo.png":      ("image/png",                 2592000),  # ç¼“å­˜30å¤©
    "metadata.json": ("application/json",           86400),   # ç¼“å­˜1å¤©
    "dictionary.db": ("application/vnd.sqlite3",    86400),   # ç¼“å­˜1å¤©
    "media.db":      ("application/vnd.sqlite3",    2592000), # ç¼“å­˜30å¤©
}


@app.get("/download/{dict_id}/file/{filename}")
async def download_file(dict_id: str, filename: str):
    """ä¸‹è½½è¯å…¸æ–‡ä»¶"""
    if filename not in ALLOWED_FILES:
        raise HTTPException(status_code=400, detail=f"File '{filename}' not allowed")

    file_path = DICTIONARIES_PATH / dict_id / filename
    if not file_path.exists():
        raise HTTPException(status_code=404, detail=f"File '{filename}' not found for dictionary '{dict_id}'")

    media_type, max_age = ALLOWED_FILES[filename]
    return FileResponse(
        path=str(file_path),
        media_type=media_type,
        filename=filename,
        headers={
            "Cache-Control": f"public, max-age={max_age}"
        }
    )


class EntryIdsRequest(BaseModel):
    entries: list[int]


@app.post("/download/{dict_id}/entries")
async def download_entries_batch(dict_id: str, data: EntryIdsRequest):
    """æŒ‰ entry_id åˆ—è¡¨æ‰¹é‡ä¸‹è½½æ¡ç›®ï¼Œè¿”å› .zst å‹ç¼©çš„ JSONL æ–‡ä»¶"""
    dict_path = DICTIONARIES_PATH / dict_id
    if not dict_path.exists():
        raise HTTPException(status_code=404, detail=f"Dictionary '{dict_id}' not found")

    db_path = dict_path / "dictionary.db"
    if not db_path.exists():
        raise HTTPException(status_code=404, detail="dictionary.db not found")

    if not data.entries:
        raise HTTPException(status_code=400, detail="No entries provided")

    entry_ids_str = [str(eid) for eid in data.entries]

    dctx = await get_zstd_decompressor(dict_id)

    conn = sqlite3.connect(str(db_path))
    placeholders = ",".join("?" * len(entry_ids_str))
    rows = conn.execute(
        f"SELECT entry_id, json_data FROM entries WHERE entry_id IN ({placeholders})",
        entry_ids_str
    ).fetchall()
    conn.close()

    if not rows:
        raise HTTPException(status_code=404, detail="No entries found")

    jsonl_lines = []
    for row in rows:
        entry_json = decompress_json_data(row[1], dctx)
        jsonl_lines.append(json.dumps(entry_json, ensure_ascii=False))

    jsonl_bytes = "\n".join(jsonl_lines).encode("utf-8")
    cctx = zstd.ZstdCompressor(level=3)
    compressed = cctx.compress(jsonl_bytes)

    return StreamingResponse(
        iter([compressed]),
        media_type="application/zstd",
        headers={
            "Content-Disposition": f'attachment; filename="entries.zst"',
            "Cache-Control": "no-cache",
        }
    )



@app.get("/word/{dict_id}/{word}")
async def query_word(dict_id: str, word: str, request: Request):
    """æŸ¥è¯¢å•è¯æ¥å£"""
    logger.info(f"Querying word '{word}' in dictionary '{dict_id}'")

    conn = await get_db_connection(dict_id)
    if conn is None:
        raise HTTPException(status_code=404, detail=f"Dictionary '{dict_id}' not found")

    try:
        cursor = await conn.execute(
            """
            SELECT * FROM entries
            WHERE headword = ?
               OR headword LIKE ?
            ORDER BY
                CASE WHEN headword = ? THEN 0 ELSE 1 END,
                headword
            LIMIT 50
            """,
            (word, f"{word}%", word)
        )

        rows = await cursor.fetchall()
        await cursor.close()

        dctx = await get_zstd_decompressor(dict_id)
        entries = [decompress_json_data(row['json_data'], dctx) for row in rows]

        return JSONResponse({
            "dict_id": dict_id,
            "word": word,
            "entries": entries,
            "total": len(entries)
        })
    except Exception as e:
        logger.error(f"Error querying word '{word}': {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@app.get("/entry/{dict_id}/{entry_id}")
async def query_entry(dict_id: str, entry_id: int):
    """æŸ¥è¯¢å•ä¸ªè¯æ¡æ¥å£"""
    conn = await get_db_connection(dict_id)
    if conn is None:
        raise HTTPException(status_code=404, detail=f"Dictionary '{dict_id}' not found")

    try:
        cursor = await conn.execute(
            "SELECT json_data FROM entries WHERE entry_id = ?",
            (entry_id,)
        )
        row = await cursor.fetchone()
        await cursor.close()

        if row is None:
            raise HTTPException(status_code=404, detail=f"Entry '{entry_id}' not found")

        dctx = await get_zstd_decompressor(dict_id)
        return JSONResponse(decompress_json_data(row['json_data'], dctx))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error querying entry '{entry_id}': {e}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@app.get("/dictionaries")
async def list_dictionaries():
    """
    è·å–è¯å…¸å•†åº—åˆ—è¡¨
    è¿”å›æ‰€æœ‰å¯ç”¨è¯å…¸çš„è¯¦ç»†ä¿¡æ¯
    """
    dictionaries = []
    
    if not DICTIONARIES_PATH.exists():
        return {"dictionaries": dictionaries}
    
    for item in DICTIONARIES_PATH.iterdir():
        if item.is_dir():
            dict_info = await get_dictionary_info(item.name)
            if dict_info:
                dictionaries.append(dict_info.dict())
    
    # æŒ‰åç§°æ’åº
    dictionaries.sort(key=lambda x: x['name'])

    return {"dictionaries": dictionaries}


async def get_file_from_zip(zip_path: Path, file_path: str, media_type: str, filename: str):
    """
    ä» zip æ–‡ä»¶ä¸­æå–å•ä¸ªæ–‡ä»¶å¹¶è¿”å›ï¼ˆä½¿ç”¨ç¼“å­˜çš„ ZipFile å¯¹è±¡ï¼‰

    æ€§èƒ½ä¿è¯ï¼š
    - ä½¿ç”¨å·²æ‰“å¼€å¹¶ç¼“å­˜çš„ ZipFile å¯¹è±¡ï¼Œä¸éœ€è¦æ¯æ¬¡é‡æ–°æ‰“å¼€
    - ä½¿ç”¨ç¼“å­˜çš„ ZIP ç´¢å¼•ï¼ŒO(1) è·¯å¾„æŸ¥æ‰¾
    - ä¸ä¼šè¯»å–æ•´ä¸ª zip æ–‡ä»¶
    - ä¸ä¼šå°†æ•´ä¸ªç›®æ ‡æ–‡ä»¶è¯»å…¥å†…å­˜
    - ä½¿ç”¨ç”Ÿæˆå™¨é€å—æµå¼è¿”å›
    - å†…å­˜å ç”¨æ’å®š O(64KB)ï¼Œä¸æ–‡ä»¶å¤§å°æ— å…³

    æ€§èƒ½æå‡ï¼š
    - æœåŠ¡å¯åŠ¨å: ZIP æ–‡ä»¶å·²æ‰“å¼€ï¼Œç´¢å¼•å·²æ„å»º
    - é¦–æ¬¡è®¿é—®: <1ms (ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„ ZipFile å¯¹è±¡) âš¡âš¡âš¡
    """
    request_start = time.perf_counter()
    logger.info(f"[REQUEST] å¼€å§‹å¤„ç† ZIP æ–‡ä»¶è¯·æ±‚: {zip_path.name}/{file_path}")

    try:
        # ğŸš€ ä½¿ç”¨ç¼“å­˜çš„ç´¢å¼•ï¼ŒO(1) æŸ¥æ‰¾ï¼
        get_index_start = time.perf_counter()
        zip_index = await get_zip_index(zip_path)
        get_index_ms = (time.perf_counter() - get_index_start) * 1000
        logger.info(f"[PERF] è·å– ZIP ç´¢å¼•è€—æ—¶: {get_index_ms:.2f}ms")

        # åœ¨å­—å…¸ä¸­æŸ¥æ‰¾ï¼ŒO(1) å¤æ‚åº¦
        lookup_start = time.perf_counter()
        if file_path not in zip_index:
            raise HTTPException(
                status_code=404,
                detail=f"File '{file_path}' not found in zip archive '{zip_path.name}'"
            )
        target_path = zip_index[file_path]
        lookup_ms = (time.perf_counter() - lookup_start) * 1000
        logger.info(f"[PERF] è·¯å¾„æŸ¥æ‰¾è€—æ—¶: {lookup_ms:.2f}ms | ç›®æ ‡è·¯å¾„: {target_path}")

        # è·å–ç¼“å­˜çš„ ZipFile å¯¹è±¡
        cache_key = str(zip_path)
        zf = _zip_file_cache.get(cache_key)
        if not zf:
            # è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸º get_zip_index åº”è¯¥å·²ç»æ‰“å¼€å¹¶ç¼“å­˜äº†
            raise HTTPException(
                status_code=500,
                detail=f"ZipFile object not found in cache for {zip_path.name}"
            )

        # åˆ›å»ºæµå¼ç”Ÿæˆå™¨ï¼Œä½¿ç”¨ç¼“å­˜çš„ ZipFile å¯¹è±¡
        async def file_iterator():
            """å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œé€å—è¯»å–æ–‡ä»¶å†…å®¹"""
            iter_start = time.perf_counter()
            chunk_count = 0
            total_bytes = 0

            # ğŸš€ ç›´æ¥ä½¿ç”¨å·²æ‰“å¼€çš„ ZipFile å¯¹è±¡ï¼Œä¸éœ€è¦é‡æ–°æ‰“å¼€ï¼
            open_start = time.perf_counter()
            with zf.open(target_path) as f:
                open_ms = (time.perf_counter() - open_start) * 1000
                logger.info(f"[PERF] æ‰“å¼€ ZIP å†…æ–‡ä»¶è€—æ—¶: {open_ms:.2f}ms")

                while True:
                    chunk = f.read(65536)  # æ¯æ¬¡åªè¯» 64KB
                    if not chunk:
                        break
                    chunk_count += 1
                    total_bytes += len(chunk)
                    yield chunk

            iter_ms = (time.perf_counter() - iter_start) * 1000
            logger.info(f"[PERF] æ–‡ä»¶æµå¼è¯»å–å®Œæˆ | å—æ•°: {chunk_count} | å­—èŠ‚: {total_bytes} | è€—æ—¶: {iter_ms:.2f}ms")

        total_prepare_ms = (time.perf_counter() - request_start) * 1000
        logger.info(f"[PERF] å“åº”å‡†å¤‡å®Œæˆï¼Œå¼€å§‹æµå¼ä¼ è¾“ | æ€»å‡†å¤‡è€—æ—¶: {total_prepare_ms:.2f}ms")

        return StreamingResponse(
            file_iterator(),
            media_type=media_type,
            headers={
                "Content-Disposition": f'inline; filename="{filename}"',
                "Cache-Control": "public, max-age=2592000"
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error reading from zip file {zip_path}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to read file from zip: {str(e)}")


def get_media_type(filename: str) -> str:
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å– MIME ç±»å‹"""
    ext = filename.split('.')[-1].lower()
    media_types = {
        'mp3': 'audio/mpeg',
        'wav': 'audio/wav',
        'ogg': 'audio/ogg',
        'png': 'image/png',
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'gif': 'image/gif',
        'webp': 'image/webp'
    }
    return media_types.get(ext, 'application/octet-stream')


@app.get("/audio/{dict_id}/{file_path:path}")
async def get_audio_file(dict_id: str, file_path: str):
    """
    è·å–å•ä¸ªéŸ³é¢‘æ–‡ä»¶
    å¦‚æœ media.db å­˜åœ¨ï¼Œåˆ™ä»æ•°æ®åº“ä¸­è¯»å–
    å¦åˆ™å…¼å®¹æ—§çš„ç›®å½•ç»“æ„
    """
    request_start = time.perf_counter()
    logger.info(f"===== [AUDIO REQUEST] å¼€å§‹å¤„ç†éŸ³é¢‘è¯·æ±‚ | è¯å…¸: {dict_id} | æ–‡ä»¶: {file_path} =====")

    dict_path = DICTIONARIES_PATH / dict_id
    if not dict_path.exists():
        raise HTTPException(status_code=404, detail=f"Dictionary '{dict_id}' not found")

    media_type = get_media_type(file_path)

    # ä¼˜å…ˆä» media.db æ•°æ®åº“ä¸­è¯»å–
    media_db_path = dict_path / "media.db"
    if media_db_path.exists():
        logger.info(f"[AUDIO] ä» media.db æ•°æ®åº“è¯»å–: {file_path}")
        try:
            conn = await get_media_db_connection(dict_id)
            if not conn:
                raise HTTPException(status_code=500, detail="Failed to connect to media database")

            # æŸ¥è¯¢éŸ³é¢‘æ–‡ä»¶
            cursor = await conn.execute(
                "SELECT blob FROM audios WHERE name = ?",
                (file_path,)
            )
            row = await cursor.fetchone()
            await cursor.close()

            if not row:
                raise HTTPException(status_code=404, detail=f"Audio file '{file_path}' not found in database")

            # è·å– blob æ•°æ®
            blob_data = row[0]

            # åˆ›å»ºæµå¼ç”Ÿæˆå™¨ï¼ˆä¸ç¼“å­˜æ–‡ä»¶å†…å®¹ï¼Œæ¯æ¬¡éƒ½ä»æ•°æ®åº“è¯»å–ï¼‰
            async def audio_iterator():
                chunk_size = 65536  # 64KB
                for i in range(0, len(blob_data), chunk_size):
                    yield blob_data[i:i + chunk_size]

            total_ms = (time.perf_counter() - request_start) * 1000
            logger.info(f"===== [AUDIO REQUEST] ä»æ•°æ®åº“è¿”å› | æ€»è€—æ—¶: {total_ms:.2f}ms =====")

            return StreamingResponse(
                audio_iterator(),
                media_type=media_type,
                headers={
                    "Content-Disposition": f'inline; filename="{file_path}"',
                    "Cache-Control": "public, max-age=2592000"  # ç¼“å­˜30å¤©
                }
            )

        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"[AUDIO] ä»æ•°æ®åº“è¯»å–å¤±è´¥: {e}")
            # ç»§ç»­å°è¯•ä»ç›®å½•è¯»å–

    # å…¼å®¹æ—§çš„ç›®å½•ç»“æ„
    logger.info(f"[AUDIO] ä»ç›®å½•è¯»å–: {file_path}")
    audios_path = dict_path / "audios"
    audio_file = audios_path / file_path
    if audio_file.exists() and audio_file.is_file():
        response = FileResponse(
            path=str(audio_file),
            media_type=media_type,
            headers={
                "Cache-Control": "public, max-age=2592000"  # ç¼“å­˜30å¤©
            }
        )
        total_ms = (time.perf_counter() - request_start) * 1000
        logger.info(f"===== [AUDIO REQUEST] ä»ç›®å½•è¿”å› | æ€»è€—æ—¶: {total_ms:.2f}ms =====")
        return response

    raise HTTPException(status_code=404, detail=f"Audio file '{file_path}' not found")


@app.get("/image/{dict_id}/{file_path:path}")
async def get_image_file(dict_id: str, file_path: str):
    """
    è·å–å•ä¸ªå›¾ç‰‡æ–‡ä»¶
    å¦‚æœ media.db å­˜åœ¨ï¼Œåˆ™ä»æ•°æ®åº“ä¸­è¯»å–
    å¦åˆ™å…¼å®¹æ—§çš„ç›®å½•ç»“æ„
    """
    dict_path = DICTIONARIES_PATH / dict_id
    if not dict_path.exists():
        raise HTTPException(status_code=404, detail=f"Dictionary '{dict_id}' not found")

    media_type = get_media_type(file_path)

    # ä¼˜å…ˆä» media.db æ•°æ®åº“ä¸­è¯»å–
    media_db_path = dict_path / "media.db"
    if media_db_path.exists():
        try:
            conn = await get_media_db_connection(dict_id)
            if not conn:
                raise HTTPException(status_code=500, detail="Failed to connect to media database")

            # æŸ¥è¯¢å›¾ç‰‡æ–‡ä»¶
            cursor = await conn.execute(
                "SELECT blob FROM images WHERE name = ?",
                (file_path,)
            )
            row = await cursor.fetchone()
            await cursor.close()

            if not row:
                raise HTTPException(status_code=404, detail=f"Image file '{file_path}' not found in database")

            # è·å– blob æ•°æ®
            blob_data = row[0]

            # åˆ›å»ºæµå¼ç”Ÿæˆå™¨ï¼ˆä¸ç¼“å­˜æ–‡ä»¶å†…å®¹ï¼Œæ¯æ¬¡éƒ½ä»æ•°æ®åº“è¯»å–ï¼‰
            async def image_iterator():
                chunk_size = 65536  # 64KB
                for i in range(0, len(blob_data), chunk_size):
                    yield blob_data[i:i + chunk_size]

            return StreamingResponse(
                image_iterator(),
                media_type=media_type,
                headers={
                    "Content-Disposition": f'inline; filename="{file_path}"',
                    "Cache-Control": "public, max-age=2592000"  # ç¼“å­˜30å¤©
                }
            )

        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"[IMAGE] ä»æ•°æ®åº“è¯»å–å¤±è´¥: {e}")
            # ç»§ç»­å°è¯•ä»ç›®å½•è¯»å–

    # å…¼å®¹æ—§çš„ç›®å½•ç»“æ„
    images_path = dict_path / "images"
    image_file = images_path / file_path
    if image_file.exists() and image_file.is_file():
        return FileResponse(
            path=str(image_file),
            media_type=media_type,
            headers={
                "Cache-Control": "public, max-age=2592000"  # ç¼“å­˜30å¤©
            }
        )

    raise HTTPException(status_code=404, detail=f"Image file '{file_path}' not found")


@app.get("/auxi/{filename:path}")
async def get_auxiliary_file(filename: str):
    """
    è·å–è¾…åŠ©æ•°æ®æ–‡ä»¶
    æ”¯æŒæ‰¹é‡ä¸‹è½½è¾…åŠ©ç›®å½•ä¸­çš„ä»»ä½•æ–‡ä»¶

    Examples:
        GET /auxi/en.db - ä¸‹è½½ en.db æ–‡ä»¶
        GET /auxi/cn.db - ä¸‹è½½ cn.db æ–‡ä»¶
        GET /auxi/data/config.json - ä¸‹è½½ data/config.json
    """
    logger.info(f"[AUXI] Requesting auxiliary file: {filename}")

    # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åä¸åŒ…å«è·¯å¾„éå†æ”»å‡»
    if '..' in filename or filename.startswith('/'):
        raise HTTPException(
            status_code=400,
            detail="Invalid filename. Path traversal is not allowed."
        )

    # æ„å»ºæ–‡ä»¶è·¯å¾„
    file_path = AUXILIARY_PATH / filename

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ˜¯æ–‡ä»¶ï¼ˆä¸æ˜¯ç›®å½•ï¼‰
    if not file_path.exists() or not file_path.is_file():
        logger.warning(f"[AUXI] File not found: {file_path}")
        raise HTTPException(
            status_code=404,
            detail=f"Auxiliary file '{filename}' not found"
        )

    # è·å– MIME ç±»å‹
    media_type = get_media_type(filename)

    logger.info(f"[AUXI] Serving file: {file_path} ({media_type})")

    return FileResponse(
        path=str(file_path),
        media_type=media_type,
        filename=filename,
        headers={
            "Cache-Control": "public, max-age=86400"  # ç¼“å­˜1å¤©
        }
    )


# å…¨å±€å¼‚å¸¸å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """å…¨å±€å¼‚å¸¸å¤„ç†"""
    logger.error(f"Unhandled exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error"}
    )


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", "8080"))
    uvicorn.run(app, host="0.0.0.0", port=port)
